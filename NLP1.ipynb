{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71a3fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wikipedia\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960635f8",
   "metadata": {},
   "source": [
    "Importing wikipedia API and writing a function to get the text from wikipedia by the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7526e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('english')\n",
    "\n",
    "# Function to retrieve a text from a Wikipedia page\n",
    "def get_wikipedia_text(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    \n",
    "    if not page.exists():\n",
    "        return None\n",
    "    \n",
    "    return page.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425e6ad",
   "metadata": {},
   "source": [
    "From the documents we recognize the mostly repeated words and consider them as keywords and the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6deba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords using NLTK\n",
    "def extract_keywords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275d4dc",
   "metadata": {},
   "source": [
    "From  the keywords we ectract the nouns using NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ec4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract nouns from keywords using NLTK\n",
    "def extract_nouns(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    nouns = [word.lower() for word, pos in tagged_words if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1509df",
   "metadata": {},
   "source": [
    "We use the 'FreqDist' class from NLTK to choose the 10 most common keywords for both medical and non-medical topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbd6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract top nouns for a given set of topics\n",
    "def extract_top_nouns(topics, num_top_nouns=10):\n",
    "    all_nouns = []\n",
    "\n",
    "    for topic in topics:\n",
    "        text = get_wikipedia_text(topic)\n",
    "        if text:\n",
    "            all_nouns.extend(extract_nouns(text))\n",
    "\n",
    "    nouns_freq_dist = FreqDist(all_nouns)\n",
    "    top_nouns = [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n",
    "\n",
    "    return top_nouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3f751",
   "metadata": {},
   "source": [
    "We use some sampled annotated keywords which we got from wikipedia in order to collect documents for medical and non_medical topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28e955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample annotated keywords for medical and non-medical topics\n",
    "medical_topics = ['diabetes', 'blood pressure', 'cancer', 'heart disease', 'vaccine', 'pandemic']\n",
    "non_medical_topics = ['programming', 'technology', 'history', 'art', 'sports', 'entertainment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9250f7b",
   "metadata": {},
   "source": [
    "For each documents(medical or non_medical) there are some words which are repeated more and can be used to seperate the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8824dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top medical and non-medical nouns\n",
    "top_medical_keywords = extract_top_nouns(medical_topics, num_top_nouns=10)\n",
    "top_non_medical_keywords = extract_top_nouns(non_medical_topics, num_top_nouns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b038405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Medical Nouns: ['cancer', 'disease', 'pressure', 'blood', 'risk', 'diabetes', 'vaccine', 'vaccines', 'people', 'heart']\n"
     ]
    }
   ],
   "source": [
    "# Display the top medical nouns\n",
    "print(\"Top Medical Nouns:\", top_medical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d33cd53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Non Medical Nouns: ['art', 'history', 'entertainment', 'example', 'sports', 'century', 'technology', 'sport', 'forms', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Display the top non medical nouns\n",
    "print(\"Top Non Medical Nouns:\", top_non_medical_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b39431",
   "metadata": {},
   "source": [
    "-Documents related to medical topics are labeled as 1, and documents related to non-medical topics are labeled as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaaa8a2",
   "metadata": {},
   "source": [
    "-CountVectorizer is then used with vocabulary=all_top_nouns to create a bag-of-words representation of the all_docs (documents) using only the specified vocabulary.By doing this, we are creating a feature matrix X where each row corresponds to a document, and each column corresponds to the count of occurrences of a specific top noun in that document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48137248",
   "metadata": {},
   "source": [
    "-This approach allows us to focus on a subset of important words (top nouns), making the feature matrix more manageable and potentially more informative for your classification task. Adjusting the num_top_nouns parameter when extracting top nouns allows us to control the number of features included in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a94da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = medical_topics + non_medical_topics\n",
    "\n",
    "# Retrieve and preprocess text for all topics\n",
    "\n",
    "all_docs = []\n",
    "all_labels = []\n",
    "\n",
    "for topic in all_topics:\n",
    "    text = get_wikipedia_text(topic)\n",
    "    if text:\n",
    "        nouns = extract_nouns(text)\n",
    "        all_docs.append(\" \".join(nouns))\n",
    "        all_labels.append(1 if topic in medical_topics else 0)\n",
    "\n",
    "# Extract features using top medical and non-medical nouns\n",
    "\n",
    "all_top_keywords = top_medical_keywords + top_non_medical_keywords\n",
    "vectorizer = CountVectorizer(vocabulary=all_top_keywords)\n",
    "X = vectorizer.transform(all_docs)\n",
    "y = all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9914d",
   "metadata": {},
   "source": [
    "Now we split the data for training and testing using Sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4723135",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f6408",
   "metadata": {},
   "source": [
    "First we are using a naive classifier for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d0a6a",
   "metadata": {},
   "source": [
    "One common type of Naive Classifier is the Majority Class Classifier, which predicts the majority class for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dd8cfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Classifier Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#a majority class naive classifier \n",
    "class NaiveClassifier:\n",
    "    def __init__(self):\n",
    "        self.majority_class = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Find the majority class in the training data\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        self.majority_class = unique_classes[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the majority class for all instances in the test data\n",
    "        return np.full(X.shape[0], self.majority_class)\n",
    "\n",
    "# Example usage\n",
    "naive_classifier = NaiveClassifier()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "naive_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "naive_predictions = naive_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, naive_predictions)\n",
    "print(\"Naive Classifier Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a71174",
   "metadata": {},
   "source": [
    "We can also use a logistic regression model from Sklearn with better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "539b41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70dacea",
   "metadata": {},
   "source": [
    "Now that our classification model is ready we can use it to classify a significant document. I downloaded a medical document from wikipedia(which is in the zip) and gave it to the model to classify it. we can also get it directly from wikipeida but i also wanted to add the code for converting pdf format to txt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bde0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the document is medical\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    # Add more preprocessing steps as needed\n",
    "    return text\n",
    "\n",
    "# Function to vectorize a document using the provided vectorizer\n",
    "def vectorize_document(document, vectorizer):\n",
    "    preprocessed_document = preprocess_text(document)\n",
    "    document_vectorized = vectorizer.transform([preprocessed_document])\n",
    "    return document_vectorized\n",
    "\n",
    "# Fit the Naive Classifier on the training data\n",
    "naive_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Specify the path to the PDF file on your desktop\n",
    "pdf_file_path = \"C:\\wiki\\Medicine.pdf\" \n",
    "\n",
    "# Read the content of the PDF file\n",
    "with open(pdf_file_path, 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    pdf_text = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        pdf_text += page.extract_text()\n",
    "\n",
    "# Assuming you have a vectorizer with the same vocabulary used during training\n",
    "vectorizer = CountVectorizer(vocabulary=all_top_keywords)  # Use the same vocabulary\n",
    "\n",
    "# Vectorize the PDF document\n",
    "document_vectorized = vectorize_document(pdf_text, vectorizer)\n",
    "\n",
    "# Make a prediction using the Naive Classifier\n",
    "naive_prediction = naive_classifier.predict(document_vectorized)\n",
    "\n",
    "if naive_prediction==1:\n",
    "    print('the document is medical')\n",
    "else:\n",
    "    print('the document is non-medical')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea20ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
